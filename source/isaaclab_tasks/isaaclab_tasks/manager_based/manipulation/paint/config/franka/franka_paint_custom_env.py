# franka_stack_custom_env.py íŒŒì¼ ë‚´ìš©


import torch
import omni.log # omni.log ì„í¬íŠ¸ ì¶”ê°€ (ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´)
from isaaclab.envs import ManagerBasedRLEnv
# ManagerBasedEnvCfg ëŒ€ì‹ , ë” êµ¬ì²´ì ì¸ PaintEnvCfgë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
# franka_paint_custom_env.py íŒŒì¼ì€ paint_env_cfg.py íŒŒì¼ê³¼ ë‹¤ë¥¸ ë””ë ‰í† ë¦¬ì— ìˆìœ¼ë¯€ë¡œ,
# ìƒëŒ€ ê²½ë¡œ ë˜ëŠ” ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜: isaaclab_tasks/manager_based/manipulation/paint/config/franka/
# PaintEnvCfg ìœ„ì¹˜: isaaclab_tasks/manager_based/manipulation/paint/
from isaaclab_tasks.manager_based.manipulation.paint.paint_env_cfg import PaintEnvCfg

class FrankaPaintCustomEnv(ManagerBasedRLEnv):
    """Custom environment class for Franka Paint task to store state."""

    # íƒ€ì… íŒíŠ¸ë¥¼ PaintEnvCfgë¡œ ëª…ì‹œ
    cfg: PaintEnvCfg

    def __init__(self, cfg: PaintEnvCfg, **kwargs):
        # ì†ì„±ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤
        self.all_target_local_positions = []
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        super().__init__(cfg=cfg, **kwargs)
        
        # ì´ì œ ë””ë°”ì´ìŠ¤ì™€ í™˜ê²½ ê°œìˆ˜ì— ì ‘ê·¼í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ current_target_idxë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤
        self.current_target_idx = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)
        
        # TARGET_LOCATIONSì—ì„œ íƒ€ê²Ÿ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
        target_positions_loaded = False

        
        if hasattr(self.cfg, "TARGET_LOCATIONS"):
            target_locations = self.cfg.TARGET_LOCATIONS
            for key in ["target1", "target2"]:
                if key in target_locations:
                    self.all_target_local_positions.append(
                        torch.tensor(target_locations[key], device=self.device, dtype=torch.float32)
                    )
                    target_positions_loaded = True
                else:
                    omni.log.warn(f"Target key '{key}' not found in cfg.TARGET_LOCATIONS.")
        
        # ì„œë¸ŒíƒœìŠ¤í¬ ì™„ë£Œ ìƒíƒœ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€
        self.completed_subtasks = {}
        # ì„œë¸ŒíƒœìŠ¤í¬ ì´ë¦„ ëª©ë¡ (ìˆœì„œ ì¤‘ìš”)
        self.subtask_names = ["approach_1", "approach_2"]  # ì‹¤ì œ ì„œë¸ŒíƒœìŠ¤í¬ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •
        # ì´ˆê¸° ìƒíƒœ: ëª¨ë‘ ë¯¸ì™„ë£Œ
        for subtask in self.subtask_names:
            self.completed_subtasks[subtask] = False

            
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶œë ¥
        omni.log.info(f"Initialized {len(self.all_target_local_positions)} target positions: {self.all_target_local_positions}")

    def _reset_idx(self, env_ids: torch.Tensor):
        super()._reset_idx(env_ids)
        if hasattr(self, "current_target_idx"):
            self.current_target_idx[env_ids] = 0
        for env_id in env_ids:
            for subtask in self.subtask_names:
                self.completed_subtasks[subtask] = False
        # ... (ë‹¤ë¥¸ ìƒíƒœ ë³€ìˆ˜ ë¦¬ì…‹) ...
    def update_subtask_completion(self, subtask_states):
        """
        ì„œë¸ŒíƒœìŠ¤í¬ ì™„ë£Œ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            subtask_states: ê° ì„œë¸ŒíƒœìŠ¤í¬ì˜ í˜„ì¬ ìƒíƒœë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        for i, subtask in enumerate(self.subtask_names):
            if subtask in subtask_states:
                current_state = subtask_states[subtask]
                
                # ì´ë¯¸ ì™„ë£Œëœ ì„œë¸ŒíƒœìŠ¤í¬ëŠ” ì™„ë£Œ ìƒíƒœ ìœ ì§€
                if self.completed_subtasks.get(subtask, False):
                    continue
                    
                # ì´ì „ ì„œë¸ŒíƒœìŠ¤í¬ê°€ ëª¨ë‘ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                all_previous_completed = True
                for prev_idx in range(i):
                    prev_subtask = self.subtask_names[prev_idx]
                    if not self.completed_subtasks.get(prev_subtask, False):
                        all_previous_completed = False
                        break
                
                # í˜„ì¬ ìƒíƒœê°€ Trueì´ê³  ì´ì „ ì„œë¸ŒíƒœìŠ¤í¬ê°€ ëª¨ë‘ ì™„ë£Œë˜ì—ˆë‹¤ë©´ ì´ ì„œë¸ŒíƒœìŠ¤í¬ ì™„ë£Œ ì²˜ë¦¬
                if current_state and all_previous_completed:
                    self.completed_subtasks[subtask] = True
                    print(f"ğŸ‰ Subtask '{subtask}' completed!")
    def get_current_target_local_pos(self, env_ids_query: torch.Tensor | None = None) -> torch.Tensor:
        # íƒ€ê²Ÿ ìœ„ì¹˜ê°€ ë¹„ì–´ ìˆëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
        if not hasattr(self, "all_target_local_positions") or not self.all_target_local_positions:
            # ê¸°ë³¸ê°’ ì„¤ì •
            default_scale = 5.0
            self.all_target_local_positions = [
                torch.tensor([-0.07 * default_scale, 0.0 * default_scale, 0.09 * default_scale], device=self.device, dtype=torch.float32),
                torch.tensor([0.07 * default_scale, 0.0 * default_scale, 0.09 * default_scale], device=self.device, dtype=torch.float32)
            ]
            omni.log.warn("Using default target positions in get_current_target_local_pos.")
        
        # ì¸ë±ìŠ¤ í™•ì¸
        if not hasattr(self, "current_target_idx"):
            self.current_target_idx = torch.zeros(self.num_envs, device=self.device, dtype=torch.long)
        
        if env_ids_query is None:
            indices_to_use = self.current_target_idx
        else:
            indices_to_use = self.current_target_idx[env_ids_query]
        
        # ë¹ˆ í™˜ê²½ ì²˜ë¦¬
        if indices_to_use.numel() == 0:
            return torch.empty((0, 3), device=self.device, dtype=torch.float32)
        
        # íƒ€ê²Ÿ ì„ íƒ
        selected_targets_list = []
        for idx_val_tensor in indices_to_use:
            idx_val = idx_val_tensor.item()
            if 0 <= idx_val < len(self.all_target_local_positions):
                selected_targets_list.append(self.all_target_local_positions[idx_val])
            else:
                omni.log.error(f"Invalid target index: {idx_val} for all_target_local_positions of size {len(self.all_target_local_positions)}")
                selected_targets_list.append(self.all_target_local_positions[0])  # ì²« ë²ˆì§¸ íƒ€ê²Ÿ ì‚¬ìš©
        
        # ê²°ê³¼ ë°˜í™˜
        return torch.stack(selected_targets_list)

    def advance_to_next_subtask(self, env_ids_to_advance: torch.Tensor):
        if env_ids_to_advance.numel() == 0 or not self.all_target_local_positions:
            return

        current_indices = self.current_target_idx[env_ids_to_advance]
        next_indices = current_indices + 1
        
        num_total_targets = len(self.all_target_local_positions)
        # ë‹¤ìŒ ì¸ë±ìŠ¤ê°€ ì´ íƒ€ê²Ÿ ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ í´ë¨í•‘
        effective_next_indices = torch.clamp(next_indices, max=num_total_targets - 1)
        
        self.current_target_idx[env_ids_to_advance] = effective_next_indices
        
        # ì‹¤ì œë¡œ ì¸ë±ìŠ¤ê°€ ë³€ê²½ëœ í™˜ê²½ë§Œ ë¡œê·¸ ì¶œë ¥
        advanced_mask = (effective_next_indices > current_indices) & (current_indices < num_total_targets - 1)
        if torch.any(advanced_mask):
            actual_advanced_env_ids = env_ids_to_advance[advanced_mask]
            new_indices_for_log = self.current_target_idx[actual_advanced_env_ids]
            omni.log.info(f"Envs {actual_advanced_env_ids.tolist()} advanced to next subtask index: {new_indices_for_log.tolist()}")

    # _compute_rewards, _check_termination ë“±ì˜ ë©”ì„œë“œì—ì„œ
    # self.get_current_target_local_pos() ì™€ self.advance_to_next_subtask()ë¥¼ í™œìš©

# class FrankaPaintCustomEnv(ManagerBasedRLEnv):
#     """Custom environment class for Franka Paint task to store state."""

#     # íƒ€ì…ì„ ëª…í™•íˆ í•˜ê¸° ìœ„í•´ cfg íƒ€ì…ì„ ì§€ì •í•´ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì„ íƒ ì‚¬í•­)
#     cfg: ManagerBasedEnvCfg

#     def __init__(self, cfg: ManagerBasedEnvCfg, **kwargs):
#         """Initialize the environment and the counter."""
#         # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” (í•„ìˆ˜)
#         super().__init__(cfg=cfg, **kwargs)

#         # --- ì—¬ê¸°ì— ì¹´ìš´í„° ë³€ìˆ˜ ì´ˆê¸°í™” ---
#         # ë””ë°”ì´ìŠ¤ì™€ í™˜ê²½ ê°œìˆ˜ëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” í›„ ì ‘ê·¼ ê°€ëŠ¥
#         self.eef_near_target_counter = torch.zeros(self.num_envs, device=self.device, dtype=torch.int32)
#         print(f"Initialized eef_near_target_counter on device {self.device} with shape {self.eef_near_target_counter.shape}")

#     def _reset_idx(self, env_ids: torch.Tensor):
#         """Reset the specified environment indices."""
#         # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ë¦¬ì…‹ ë¡œì§ í˜¸ì¶œ (í•„ìˆ˜)
#         super()._reset_idx(env_ids)

#         # --- í•´ë‹¹ í™˜ê²½ë“¤ì˜ ì¹´ìš´í„° ë¦¬ì…‹ ---
#         if hasattr(self, "eef_near_target_counter"):
#             self.eef_near_target_counter[env_ids] = 0
#         else:
#              # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ì´ˆê¸°í™” ì•ˆëìœ¼ë©´ ê²½ê³  (ë””ë²„ê¹…ìš©)
#              print("[Warning] Attempted to reset eef_near_target_counter before initialization.")

#     # í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì— ë‹¤ë¥¸ ì»¤ìŠ¤í…€ ë©”ì†Œë“œ ì¶”ê°€ ê°€ëŠ¥
#     # ì˜ˆ: _step(actions) ë©”ì†Œë“œë¥¼ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ ë§¤ ìŠ¤í… íŠ¹ì • ë¡œì§ ìˆ˜í–‰
#     # def _step(self, actions: torch.Tensor) -> Tuple[Dict[str, Tensor], Tensor, Tensor, Tensor, Dict[str, Any]]:
#     #     # ... pre-step logic ...
#     #     obs, rewards, terminated, truncated, info = super()._step(actions)
#     #     # ... post-step logic ...
#     #     return obs, rewards, terminated, truncated, info